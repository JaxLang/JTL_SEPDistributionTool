{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea929804-5cc7-4580-8dc8-339ec95fd0f7",
   "metadata": {},
   "source": [
    "# SEP Distribution Tool\n",
    "\n",
    "This tool downloads SEP intensity-time series data from various different spacecraft and visualizes the SEP distribution using Gaussian curves in one final results plot.\n",
    "\n",
    "This tool uses a preset proton energy range to keep the various observations comparable.\n",
    "\n",
    "There is an option to also show a spacecraft-constellation plot (Solar-MACH) and a table summarizing the spacecraft coordinates for the selected time interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82416afb-cd20-4701-83ed-f7b275c80632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import JTL_SEP_functions as jtl\n",
    "\n",
    "from seppy.util import jupyterhub_data_path\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from solarmach import SolarMACH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4909dd7c-4faf-4046-bba9-55f68af6b171",
   "metadata": {},
   "source": [
    "## Saving figures and date\n",
    "\n",
    "You can usually save a figure from the Notebook by right-clicking on it while holding down the ⇧ Shift key, then select \"Save Image As...\" (or similar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74786f29-44ec-475d-9a00-9b997bcea3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your local path where you want to save the data files. \n",
    "# If run on the project's JupyterHub server, set it to a common data folder. \n",
    "data_path = f\"{os.getcwd()}{os.sep}data/\"\n",
    "data_path = jupyterhub_data_path(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8bdbc1-ee30-4260-9c93-c9a9b3a73d08",
   "metadata": {},
   "source": [
    "## Define the event details\n",
    "\n",
    "Collect the event start and end dates (specifying the start at / near the observed flare onset time), and the eruption location (in Stonyhurst)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd54b5d7-0ca4-4397-8ec8-047397c2711d",
   "metadata": {},
   "outputs": [],
   "source": [
    "startdate = dt.datetime(2021,5,28,22,19)\n",
    "enddate = startdate + pd.Timedelta(days=1)\n",
    "dates = [startdate, enddate]\n",
    "\n",
    "source_location = [67, 19] #longitude, latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5301503d-eb25-4e33-8237-961623938cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options\n",
    "event_options = {'04Jan2025': {'date': \"2025/01/04 18:27:00\",\n",
    "                               'source': [60, -15]},\n",
    "                 '17Dec2024': {'date': \"2024/12/17 12:53:00\",\n",
    "                               'source': [33, -16]},\n",
    "                 '08Dec2024': {'date': \"2024/12/08 08:50:00\",\n",
    "                               'source': [52, -6]},\n",
    "                 '03Oct2024': {'date': \"2024/10/03 12:08:00\",\n",
    "                               'source': [8, -15]},\n",
    "                 '01Sep2024': {'date': \"2024/09/01 14:44:00\",\n",
    "                               'source': [66, -12]}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fb5266-cdb8-41ac-b161-e492552b6a95",
   "metadata": {},
   "source": [
    "## Show the fleet distribution\n",
    "For more information on the Solar-MACH tool, see: reflinkhere\n",
    "\n",
    "NB: If you wish to use BepiColombo data then it will need to downloaded separately and saved to the same folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d54ba9a-5eb0-4183-8b31-bccd5ea5cb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "solarmach_table = jtl.solarmach_basic(startdate, data_path, coord_sys='Stonyhurst', source_location=source_location)\n",
    "display(solarmach_table)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a50bc8-03ad-40c6-97b6-755ea9cb9e74",
   "metadata": {},
   "source": [
    "## Spacecraft options\n",
    "\n",
    "Here details the relevant spacecraft, their instruments, which channel(s) is(are) used, and the intercalibration values. The only intercalibration value we have found so far is from [Richardson et al. (2014), page 3064](https://doi.org/10.1007/s11207-014-0524-8) which found SOHO/ERNE-HED 13.8-24.2 MeV proton intensities to be about 1.5 times the STEREO-A/HET proton intensities of the same energy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b849901-9a67-4616-9aed-91210b180291",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 25-40 MeV Proton channels\n",
    "#proton_channels = {'PSP': {'instrument': 'EpiHi-HET',\n",
    "#                           'channels': [8,9],\n",
    "#                           'intercalibration': 1},\n",
    "#                   'SOHO': {'instrument': 'ERNE-HED',\n",
    "#                            'channels': [3,4],\n",
    "#                            'intercalibration': 1},\n",
    "#                   'STEREO-A': {'instrument': 'HET',\n",
    "#                                'channels': [5,8],\n",
    "#                                'intercalibration': 1},\n",
    "#                   'Solar Orbiter': {'instrument': 'HET',\n",
    "#                                     'channels': [19,24],\n",
    "#                                     'intercalibration': 1}}\n",
    "\n",
    "# ~14 MeV Proton channels\n",
    "proton_channels = {'PSP': {'instrument': 'EpiHi-HET',\n",
    "                           'channels': [3,4],\n",
    "                           'intercalibration': 1},\n",
    "                   'SOHO': {'instrument': 'ERNE-HED',\n",
    "                            'channels': [0],\n",
    "                            'intercalibration': 0.67},\n",
    "                   'STEREO-A': {'instrument': 'HET',\n",
    "                                'channels': [0],\n",
    "                                'intercalibration': 1},\n",
    "                   'Solar Orbiter': {'instrument': 'HET',\n",
    "                                     'channels': [10,12],\n",
    "                                     'intercalibration': 1}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776975a9-10ee-4049-9299-ea37949a8466",
   "metadata": {},
   "source": [
    "Which instruments would you like to include?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9901e9c7-4fba-45fc-ae9e-d073d406f6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacecraft = ['PSP', 'SOHO', 'STEREO-A', 'Solar Orbiter']\n",
    "sc_to_plot = spacecraft # if a spacecraft is removed from the above, but you still want it plotted.\n",
    "intercalibration = False\n",
    "radial_scaling = False\n",
    "radscaling_values = [1.97, 0.27] # values for a \\pm b ; 'p': {'a': 1.97, 'b': 0.27}\n",
    "resampling = '15min'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cfeb34-8870-45a4-98b0-c21888ac27b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = jtl.load_sc_data(spacecraft=spacecraft, \n",
    "                      proton_channels=proton_channels,\n",
    "                      dates=dates,\n",
    "                      data_path=data_path,\n",
    "                      intercalibration=intercalibration, \n",
    "                      radial_scaling=radial_scaling,\n",
    "                      resampling=resampling,\n",
    "                      reference_loc=source_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86461784-dab3-431f-b45c-3fa0347471fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n",
    "jtl.plot_timeseries_result(df, data_path, dates)\n",
    "\n",
    "\n",
    "# Build to one df (synced the times) with headers:\n",
    "# Layer 1: sc-ins\n",
    "# Layer 2: Flux, Uncertainty, radial position, longitude."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736db591-d37d-4e27-bfbf-3355cd777ed4",
   "metadata": {},
   "source": [
    "## Instrument intercalibration\n",
    "\n",
    "There aren't many recent studies done on calculating the proton instruments intercalibration but most HET instruments are assumed to be similar. The default values used here work on that assumption and use the [Richardson et al.(2014)](https://link.springer.com/article/10.1007/s11207-014-0524-8) study for the intercalibration factor between SOHO-ERNE/HED and STEREO-A HET (1 : 1.5)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a72ffb4-220c-4bc7-9773-d5e4dd9fa459",
   "metadata": {},
   "source": [
    "def intercalibration_calculation(df, observer_metadict, data_path, dates):\n",
    "    # Iterate through the observers (first header in df)\n",
    "    for obs, meta_data in observer_metadict.items():\n",
    "        factor = meta_data['intercalibration'] # extract the intercalibration factor\n",
    "        print(obs)\n",
    "        print(meta_data)\n",
    "        print(factor)\n",
    "        jax=input('yes? ')\n",
    "\n",
    "        # Apply the scaling to the Flux and Uncertainty columns\n",
    "        for col in ['Flux','Uncertainty']: # Both are calculated the same\n",
    "            print(df[(obs,col)])\n",
    "            jax=input('huh?')\n",
    "            df[(obs, col)] *= factor\n",
    "\n",
    "    df.to_csv(f\"{data_path}SEP_intensities_{dates[0].strftime(\"%d%m%Y\")}_IC.csv\") # Save for sanity checks\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd87592-c43d-48c8-8e3f-076e1d9aacba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = jtl.intercalibration_calculation(df, proton_channels, data_path, dates)\n",
    "jtl.plot_timeseries_result(df1, data_path, dates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d221b1d9-17c6-4e1a-86aa-e7234ea256c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc01e1f-1d1e-4b35-9265-813ee08978d2",
   "metadata": {},
   "source": [
    "## Radial Scaling\n",
    "\n",
    "Using the values presented in [Farwa, et al. (2025)](https://www.aanda.org/articles/aa/abs/2025/01/aa50945-24/aa50945-24.html), which used values for 27-37 MeV protons from [Lario et al. (2006)](https://iopscience.iop.org/article/10.1086/508982) (for ~100 keV electrons, [Rodríguez-García et al. (2023)](https://www.aanda.org/10.1051/0004-6361/202244553) is used).\n",
    "\n",
    "The scaled intensity is calculated as $I_{1 au} = I \\cdot R^{a\\pm b}$, where $R$ is the radial distance, $I$ is the original intensity, and (for protons specifically) the scaling factors are given as $a \\pm b = 1.97 \\pm 0.27$.\n",
    "\n",
    "To calculate the scaled uncertainty, we use the following procedure:\n",
    "1. Calculate the boundary limits for the intensity calculation (e.g. $I_+ = I\\cdot R^{a+b}$; $I_- = I\\cdot R^{a-b}$; Therefore, $\\Delta I_+ = |I_{1 au}-I_+|$ and $\\Delta I_- = |I_{1 au}-I_-|$ are the limits.\n",
    "2. Find the higher boundary limit, as long as it is < the nominal value ($I_{1 au}$).\n",
    "3. Calculate the scaled uncertainty value: $\\Delta I_{1 au} = \\Delta I \\cdot R^a$.\n",
    "4. Combine both to get a final uncertainty value: $\\Delta I_{1 au, final} = \\sqrt{(\\beta)^2 + (\\Delta I_{1 au})^2}$.\n",
    "\n",
    "NB: Check that this final uncertainty is still less than the intensity value!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e865ca8a-f0d3-4668-a5f7-900ef586148c",
   "metadata": {},
   "source": [
    "def radial_scaling_calculation(df0, data_path, scaling_values, dates):\n",
    "    df = df0.copy(deep=True) # so it doesnt mess with the OG df's\n",
    "    \n",
    "    a = scaling_values[0]\n",
    "    b = scaling_values[1]\n",
    "\n",
    "    # Iterate through observers\n",
    "    for obs, df_obs in df.groupby(level=0, axis=1): # returning the observer and their specific df\n",
    "\n",
    "        for t in df_obs.index:\n",
    "            print(df_obs.loc[t, (obs,'Flux')])\n",
    "            if pd.isna(df_obs.loc[t, (obs,'Flux')]) or pd.isna(df_obs.loc[t, (obs,'r_dist')])\\\n",
    "            or (df_obs.loc[t, (obs,'Flux')]==0) or (df_obs.loc[t, (obs,'r_dist')]==0):\n",
    "                f_rscld = np.nan\n",
    "                unc_final = np.nan\n",
    "            \n",
    "            else:\n",
    "                # Scale the flux\n",
    "                f_rscld = df_obs.loc[t, (obs,'Flux')] * (df_obs.loc[t, (obs,'r_dist')] ** a)\n",
    "\n",
    "                # Scale the uncertainty\n",
    "                ## Find the difference from the boundaries\n",
    "                unc_plus = df_obs.loc[t, (obs,'Flux')] * (df_obs.loc[t, (obs,'r_dist')] **(a+b))\n",
    "                unc_limit_plus = abs(f_rscld - unc_plus)\n",
    "                unc_minus = df_obs.loc[t, (obs,'Flux')] * (df_obs.loc[t, (obs,'r_dist')] **(a-b))\n",
    "                unc_limit_minus = abs(f_rscld - unc_minus)\n",
    "    \n",
    "                if (unc_limit_plus >= unc_limit_minus) and (f_rscld - unc_limit_plus > 0):\n",
    "                    chosen_unc_limit = unc_limit_plus\n",
    "                elif (unc_limit_minus >= unc_limit_plus) and (f_rscld - unc_limit_minus > 0):\n",
    "                    chosen_unc_limit = unc_limit_minus\n",
    "                else:\n",
    "                    print(\"There's a problem with the limits\")\n",
    "                    print(\"OG flux: \", df_obs.loc[t, (obs,'Flux')])\n",
    "                    print(\"OG rad: \", df_obs.loc[t, (obs,'r_dist')])\n",
    "                    print(\"Scaled Flux: \", f_rscld)\n",
    "                    print(\"Unc plus: \", unc_limit_plus)\n",
    "                    print(\"Unc minus: \", unc_limit_minus)\n",
    "                    jax = input('Continue? ')\n",
    "                    chosen_unc_limit = np.nan\n",
    "    \n",
    "                ## Find the calculated scaled uncertainty\n",
    "                unc_calculated = df_obs.loc[t, (obs,'Uncertainty')] * (df_obs.loc[t, (obs,'r_dist')] ** a)\n",
    "    \n",
    "                ## Merge both results for the final scaled uncertainty\n",
    "                unc_final = np.sqrt((unc_calculated)**2 + (chosen_unc_limit)**2)\n",
    "\n",
    "            df.loc[t, (obs, 'Flux')] = f_rscld\n",
    "            df.loc[t, (obs, 'Uncertainty')] = unc_final\n",
    "\n",
    "\n",
    "    df.to_csv(f\"{data_path}SEP_intensities_{dates[0].strftime(\"%d%m%Y\")}_RS.csv\") # Save for sanity checks\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20196d3-981e-4ec0-aa81-e1eab88580bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2 = jtl.radial_scaling_calculation(df1, data_path, radscaling_values, dates)\n",
    "jtl.plot_timeseries_result(df2, data_path, dates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0655ba2-8331-4955-8937-26be6c985832",
   "metadata": {},
   "source": [
    "## Background subtraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b955c24-54f9-4d61-9943-3379296a6515",
   "metadata": {},
   "source": [
    "plot time series of all instruments and let the user decide on a background window for each. Then background subtract it all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e529ad-1532-4ecb-b9fa-e577836fe74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def background_subtracting(df, data_path, background_window):\n",
    "    \"\"\"Input: the full dataframe, the path for saving files, the background window.\n",
    "    Process: 1. Find the nanmean  and nanstd of the background window for both flux and unc.\n",
    "            2. Subtract the full column by the [avg - std] (this way there's less chance to get half the values as nan).\n",
    "                - The unc is calculated as: unc_adj = sqrt(unc**2 + [avg-std]**2)\n",
    "            3. Return the updated df.\"\"\"\n",
    "    # Iterate through the big df\n",
    "    for obs, obs_df in df.groupby(level=0, axis=1): # Returns the observer and their specific df\n",
    "        print(obs)\n",
    "        print(obs_df.head())\n",
    "        # A list of just the values within the background window\n",
    "        bg_flux = obs_df[(obs,'Flux')][background_window[0]:background_window[1]]\n",
    "        bg_func = obs_df[(obs,'Uncertainty')][background_window[0]:background_window[1]]\n",
    "\n",
    "        # Find the nanmean and nanstd\n",
    "        f_bg_avg = float(np.nanmean(bg_flux)) - float(np.nanstd(bg_flux, ddof=1))\n",
    "        u_bg_avg = float(np.nanmean(bg_func)) - float(np.nanstd(bg_func, ddof=1))\n",
    "        print('flux bg avg - std: ', f_bg_avg)\n",
    "        print('unc bg avg - std: ', u_bg_avg)\n",
    "        jax=input('good?')\n",
    "\n",
    "        # Adjust the whole column\n",
    "        adj_flux = (obs_df.loc[:, (obs,'Flux')]) - f_bg_avg\n",
    "        adj_func = np.sqrt( ((obs_df.loc[:, (obs,'Uncertainty')])**2) + (u_bg_avg**2) )\n",
    "\n",
    "        # Replace any negative results with 'nan'\n",
    "        adj_flux = adj_flux.where(adj_flux>=0, np.nan) # new list = old list where the values are >=0, else make the value 'nan'\n",
    "        adj_func = adj_func.where(adj_func>=0, np.nan)\n",
    "\n",
    "        # Put the adjusted column in\n",
    "        df[(obs,'Flux')] = adj_flux\n",
    "        df[(obs,'Uncertainty')] = adj_func\n",
    "\n",
    "    df.to_csv(data_path+'backsubtest.csv', na_rep='nan')\n",
    "    \n",
    "    return df\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25077d3-55be-409d-9c96-9257c1fd44ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot\n",
    "\n",
    "# User input on background\n",
    "background_window = [startdate-dt.timedelta(hours=2), startdate+dt.timedelta(minutes=60)]\n",
    "jtl.plot_timeseries_result(df2, data_path, dates, background_window=background_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcfa17c-d2e6-4166-897a-facba566e0c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Background subtraction\n",
    "print(background_window)\n",
    "df3 = background_subtracting(df2, data_path, background_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75377708-36c8-47f3-8ffb-ecaf0af1e422",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jtl.plot_timeseries_result(df3, data_path, dates, background_window=background_window)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76345382-2df6-4324-8c7c-30ebd6c5e938",
   "metadata": {},
   "source": [
    "# Gaussian curve fitting\n",
    "First with scipy.curve_fit then with scipy's ODR function (with the uncertainties). Produce and save a fig into a new subfolder each time. Fig should include curve on the left and intensity with vertical line for time tracking on the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f5b71f-8ad4-49cc-b499-9760e0889a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54ef903e-4ffd-444f-a6ae-c1b04976526e",
   "metadata": {},
   "source": [
    "# Plot final time series\n",
    "3 subplots:\n",
    "1. Intensity\n",
    "2. Center\n",
    "3. Sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee4a30b-9022-4a77-8e1a-f8ae3e1f06e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e705c025-aee9-477f-8db6-efe02e52e153",
   "metadata": {},
   "source": [
    "# (Optional) Gif of gaussian figs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c4afb9-3815-4262-b81c-7ceb299fccd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6a00f8a-6a69-418f-819d-84988d61b6ac",
   "metadata": {},
   "source": [
    "# (For later) Latitude fits \n",
    "Fit ecliptic plane instruments first (instruments at < 10 degrees latitude) then plot all instruments with ecliptic gaussian to deduce offset. Still to find events for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fae8936-f658-47a3-b8e3-d7f1de9646f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80102e13-fbdc-4916-bd64-554706125190",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6968b9-09d5-4da4-b282-02110b92dced",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
